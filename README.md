# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #3 выполнил:
- Волков Кирилл Дмитриевич
- РИ210914
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения.
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения.
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения.
- Выводы.

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1. Реализовать систему машинного обучения в связке Python - Google-Sheets – Unity.
Создание проекта Unity и добавление в него пакетов ML Agent.

<img width="362" alt="Снимок экрана 2022-10-22 в 18 15 59" src="https://user-images.githubusercontent.com/79734984/197768538-a54aa5d9-f31f-4619-a309-9be94ad0c9fc.png">

Серия команд для создания и активации нового ML-агента, а также для скачивания необходимых библиотек:

```
conda create -n MLAgent python=3.6
```

```
conda activate MLAgent
```

```
pip install mlagents
```

```
pip install torch==1.7.1+cu110 --extra-index-url https://download.pytorch.org/whl/cu110
```
Создание на сцене плоскости, куба и сферы
<img width="1329" alt="Снимок экрана 2022-10-22 в 19 30 28" src="https://user-images.githubusercontent.com/79734984/197770689-0ede7400-373a-4048-9a7e-5e869723b76b.png">

Добавление приведенного ниже скрипта в RollerAgent.cs
```C#
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;

public class RollerAgent : Agent
{
    Rigidbody rBody;
    void Start()
    {
        rBody = GetComponent<Rigidbody>();
    }

    public Transform Target;
    public override void OnEpisodeBegin()
    {
        if (transform.localPosition.y < 0)
        {
            rBody.angularVelocity = Vector3.zero;
            rBody.velocity = Vector3.zero;
            transform.localPosition = new Vector3(0, 0.5f, 0);
        }

        Target.localPosition = new Vector3(Random.value * 8-4, 0.5f, Random.value * 8-4);
    }
    public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(Target.localPosition);
        sensor.AddObservation(this.transform.localPosition);
        sensor.AddObservation(rBody.velocity.x);
        sensor.AddObservation(rBody.velocity.z);
    }
    public float forceMultiplier = 10;
    public override void OnActionReceived(ActionBuffers actionBuffers)
    {
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = actionBuffers.ContinuousActions[0];
        controlSignal.z = actionBuffers.ContinuousActions[1];
        rBody.AddForce(controlSignal * forceMultiplier);

        float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

        if(distanceToTarget < 1.42f)
        {
            SetReward(1.0f);
            EndEpisode();
        }
        else if (transform.localPosition.y < 0)
        {
            EndEpisode();
        }
    }
}
```

Добавление объекту «сфера» компонентов Rigidbody, Decision Requester, Behavior Parameters и их настройка
<img width="410" alt="Снимок экрана 2022-10-25 в 17 35 57" src="https://user-images.githubusercontent.com/79734984/197774831-562d6780-4654-4361-be3e-7eeae8276194.png">

Запуск агента и [видео сцены с одной моделью «Плоскость-Сфера-Куб»](https://youtu.be/8TR0AsTdoAM)

<img width="871" alt="Снимок экрана 2022-10-25 в 17 32 02" src="https://user-images.githubusercontent.com/79734984/197776257-2beb2f11-68c2-4b9f-ad5a-be8dbcbab767.png">

[Видео запуска сцены с 27 моделями «Плоскость-Сфера-Куб»](https://youtu.be/-nKOP1dtIPc)

[Видео проверки работы модели](https://youtu.be/bmh5rgU94Bo)

### Выводы
Чем больше моделей обучается одновременно, тем быстрее буден виден результат обучения

## Задание 2

Описание строк файла конфигурации нейронной сети

| **Параметр** | **Описание** |
| :----------- | :----------- |
| `trainer_type` | Тип тренажера для использования, по умолчанию обучение с наградой |
| `batch_size` | Количество опытов на каждой итерации градиентного спуска. Всегда должно быть в несколько раз меньше, чем `buffer_size`. Если используются непрерывные действия, это значение должно быть большим (порядка 1000). Если используются только дискретные действия, это значение должно быть меньше (порядка 10 секунд) |
| `buffer_size` | Количество опыта, который необходимо собрать перед обновлением модели политики. Соответствует тому, сколько опыта должно быть собрано, прежде чем мы приступим к какому-либо изучению или обновлению модели. Должно быть в несколько раз больше, чем `batch_size`. Обычно больший размер буфера соответствует более стабильным обновлениям обучения |
| `learning_rate` | Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска. Обычно это значение следует уменьшить, если тренировка нестабильна, а вознаграждение постоянно не увеличивается |
| `beta` | Регулировка энтропии, которая делает политику "более случайной". Это гарантирует, что агенты должным образом исследуют пространство действий во время обучения. Увеличение этого параметра обеспечит выполнение большего количества случайных действий |
| `epsilon` | Влияет на то, насколько быстро политика может развиваться во время обучения. Соответствует допустимому порогу расхождения между старой и новой политиками при обновлении с градиентным спуском. Установка малого значения приведет к более стабильным обновлениям, но также замедлит процесс обучения. |
| `lambd` | Параметр регуляризации, используется при расчете обобщенной оценки преимущества GAE. Это можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при расчете обновленной оценки стоимости. Низкие значения соответствуют тому, что агент больше полагается на текущую оценку стоимости, а высокие значения соответствуют тому, что агент больше полагается на фактические вознаграждения, полученные в среде  |
| `num_epoch` | Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. Чем больше размер пакета, тем больше будет проходов. Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения. |
| `learning_rate_schedule` | Определяет, как скорость обучения меняется с течением времени. Для `PPO` рекомендуется снижать скорость обучения до `max_steps`, чтобы обучение проходило более стабильно.  |
| `normalize` | Применяется ли нормализация к входным данным векторного наблюдения. Эта нормализация основана на текущем среднем значении и дисперсии векторного наблюдения. Нормализация может быть полезна в случаях со сложными проблемами непрерывного управления, но может быть вредной при решении более простых задач дискретного управления |
| `hidden_units` | Количество узлов в скрытых слоях нейронной сети. Соответствует количеству узлов в каждом полностью связном слое нейронной сети. Для простых задач, где правильное действие представляет собой простую комбинацию входных данных наблюдения, параемтр должен принимать небольшое значение. Для задач, где действие представляет собой очень сложное взаимодействие между переменными наблюдения, параемтр должен принимать значение больше |
| `num_layers` | Количество скрытых слоев в нейронной сети. Соответствует тому, сколько скрытых слоев присутствует после ввода наблюдения или после CNN кодирования визуального наблюдения. Для простых задач меньшее количество слоев, скорее всего, будет обучаться быстрее и эффективнее. Для более сложных задач управления может потребоваться больше слоев. |
| `gamma` | Коэффициент дисконтирования для будущих вознаграждений, поступающих от окружения. Это можно рассматривать как то, насколько далеко в будущем агент должен заботиться о возможных вознаграждениях. В ситуациях, когда агент должен действовать в настоящем, чтобы подготовиться к вознаграждению в будущем, это значение должно быть большим. В тех случаях, когда вознаграждение приходит быстрее, оно может быть меньше. Должно быть строго меньше 1 |
| `strength` | Коэффициент, на который можно умножить вознаграждение, получаемое от окружения. Обычно диапазоны варьируются в зависимости от сигнала вознаграждения. |
| `max_steps` | Общее количество шагов, которые должны быть выполнены в среде (или во всех средах, если используется несколько параллельно) до завершения процесса обучения. Если есть несколько агентов с одинаковым именем в среде, все шаги, выполняемые этими агентами, будут способствовать одинаковому количеству `max_steps` |
| `time_horizon` | Сколько шагов опыта нужно собрать каждому агенту, прежде чем добавить его в буфер опыта. Когда этот предел достигается до окончания эпизода, оценка стоимости используется для прогнозирования общего ожидаемого вознаграждения от текущего состояния агента. Таким образом, этот параметр балансирует между менее предвзятой, но более высокой оценкой дисперсии (длительный временной горизонт) и более предвзятой, но менее разнообразной оценкой (короткий временной горизонт). В тех случаях, когда в эпизоде часто бывают награды или эпизоды непомерно велики, меньшее количество может быть более идеальным. Это число должно быть достаточно большим, чтобы охватить все важное поведение в последовательности действий агента |
| `summary_freq` | Количество опыта, который необходимо собрать перед созданием и отображением статистики обучения. Определяет степень детализации графиков в Tensorboard |

Информация о компонентах

| **Компонента** | **Описание** |
| :----------- | :----------- |
| `Decision Requester,` | При обучении с подкреплением обученная система сначала должна принять решение, прежде чем она сможет предпринять действие. Целью `Decision Requester` является извлечение и отправка вычисленного числового значения, которое формирует ответ в данный момент времени. По отношению к агентам ML это совокупность буферов дискретных и непрерывных действий. Вызывается через определенные промежутки времени, которые можно задать с помощью полосы прокрутки периода принятия решения |
| `Behavior Parameters` | Определяет, как агент принимает решения |
| `Behavior Parameters -> Vector Observation Space` | Прежде чем принять решение, агент собирает свои наблюдения о своем положении в мире. Векторное наблюдение - это вектор чисел с плавающей запятой, которые содержат релевантную информацию для принятия агентом решений. Если в параметрах указано `Space Size = 8`, это означает, что вектор признаков, содержащий наблюдения агента, содержит восемь элементов: компоненты x и z вращения куба агента и компоненты x, y и z относительного положения и скорости шара. |
| `Behavior Parameters -> Actions` | Агенту даются инструкции в форме действий. ML-Agents Toolkit классифицирует действия на два типа: непрерывные и дискретные |


## Задание 3

## Выводы

В данной лабораторной работе я научился создавать и записывать данные в google таблицы, а также получать и использовать их в Unity в разных сценариях

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
